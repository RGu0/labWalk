{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import shutil\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.ndimage import gaussian_filter\n",
    "from sklearn.utils import check_random_state\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import svm, metrics\n",
    "import datetime as dt\n",
    "\n",
    "import wfdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawData = []\n",
    "label = [];\n",
    "\n",
    "for i in range(45):\n",
    "    index = '{:03}'.format(i)\n",
    "    fileName = 'LabWalks/co' + index + '_base'\n",
    "    if os.path.isfile(fileName + '.hea'):\n",
    "        record = wfdb.rdrecord(fileName)\n",
    "        data = record.p_signal\n",
    "        rawData.append(data)\n",
    "        label.append(0)\n",
    "    \n",
    "    index = '{:03}'.format(i)\n",
    "    fileName = 'LabWalks/fl' + index + '_base'\n",
    "    if os.path.isfile(fileName + '.hea'):\n",
    "        record = wfdb.rdrecord(fileName)\n",
    "        data = record.p_signal\n",
    "        rawData.append(data)\n",
    "        label.append(1)\n",
    "\n",
    "index = 3\n",
    "print(np.shape(rawData[index]))\n",
    "plt.plot(rawData[index][:,0:3])\n",
    "a = gaussian_filter(rawData[index], sigma = 10)\n",
    "#plt.plot(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check One Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import find_peaks\n",
    "\n",
    "for index in range(1):\n",
    "    one = rawData[index]\n",
    "    Fs = 100;\n",
    "    for i in range(3):\n",
    "        timeSeq = one[:, i] - np.mean(one[:, i])\n",
    "        N = len(timeSeq)\n",
    "        frqSeq = np.fft.fft(timeSeq)\n",
    "        frqSeqShift = np.fft.fftshift(frqSeq)\n",
    "        #print(type(frqSeq))\n",
    "        frqSeqHalf = frqSeq[:N//2+1]\n",
    "        psd = 1/(Fs*N) * abs(frqSeqHalf)**2\n",
    "        psd[2:] = psd[2:]*2\n",
    "        freq = np.linspace(0,Fs/2,N//2+1)\n",
    "        plt.figure()\n",
    "        plt.plot(freq, abs(psd), lw=2)\n",
    "        peaks, _ = find_peaks(psd)\n",
    "        plt.plot(freq[peaks], psd[peaks], \"x\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NumOfSamples = 1000\n",
    "\n",
    "one = rawData[5]\n",
    "\n",
    "accNorm = np.linalg.norm(one[:,0:3],axis=1)\n",
    "gyrNorm = np.linalg.norm(one[:,3:],axis=1)\n",
    "refData = gaussian_filter(accNorm, sigma = 10)\n",
    "#check\n",
    "checkData = []\n",
    "checkTime = []\n",
    "checkDataVar = []\n",
    "#results\n",
    "oneTime = []\n",
    "oneIMUdata = [[] for i in range(6)]\n",
    "\n",
    "index = 0\n",
    "while index + 1 < len(one):\n",
    "    startIndex = index\n",
    "    #print(startIndex)\n",
    "    while index + 1 < len(one) and refData[index] < max(refData[index-1], refData[index+1]):\n",
    "        index = index + 1\n",
    "    index = index + 1\n",
    "    endIndex = index  \n",
    "    #print(endIndex)\n",
    "    if endIndex - startIndex > 20:\n",
    "        checkTime.append(endIndex - startIndex)\n",
    "\n",
    "    if endIndex < len(one) - 1 and endIndex - startIndex > 0.8*np.mean(checkTime) and endIndex - startIndex < 1.2*np.mean(checkTime):            \n",
    "        usedData = accNorm[startIndex: endIndex]\n",
    "        f = interp1d(range(len(usedData)), usedData, kind='cubic')\n",
    "        x = np.linspace(0, len(usedData)-1, num=NumOfSamples, endpoint=False)\n",
    "        processedData = f(x)\n",
    "        checkDataVar.append(np.var(processedData))\n",
    "        if(np.var(processedData) > 0.5 * np.mean(checkDataVar)):\n",
    "            oneTime.append(endIndex - startIndex)\n",
    "            checkData.append(processedData)\n",
    "            for i in range(6):\n",
    "                usedData = one[startIndex: endIndex, i]\n",
    "                f = interp1d(range(len(usedData)), usedData, kind='cubic')\n",
    "                x = np.linspace(0, len(usedData)-1, num=NumOfSamples, endpoint=False)\n",
    "                oneIMUdata[i].append(f(x))    \n",
    "\n",
    "checkData = np.array(checkData)\n",
    "print(len(checkData))\n",
    "\n",
    "    \n",
    "checkData = np.array(checkData)\n",
    "print(np.mean(checkTime))\n",
    "print(len(checkData))\n",
    "print(np.std(checkData, axis=1))\n",
    "\n",
    "print(np.shape(np.array(oneIMUdata)))\n",
    "\n",
    "plt.plot(np.std(np.array(checkData),axis=0)/np.mean(np.array(checkData),axis=0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Period Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NumOfSamples = 1000\n",
    "\n",
    "IMUdata = []\n",
    "allTime = []\n",
    "\n",
    "for one in rawData:\n",
    "\n",
    "    accNorm = np.linalg.norm(one[:,0:3],axis=1)\n",
    "    gyrNorm = np.linalg.norm(one[:,3:],axis=1)\n",
    "    refData = gaussian_filter(accNorm, sigma = 10)\n",
    "    #check\n",
    "    checkData = []\n",
    "    checkTime = []\n",
    "    checkDataVar = []\n",
    "    #results\n",
    "    oneTime = []\n",
    "    oneIMUdata = [[] for i in range(6)]\n",
    "    \n",
    "    index = 0\n",
    "    while index + 1 < len(one):\n",
    "        startIndex = index\n",
    "        #print(startIndex)\n",
    "        while index + 1 < len(one) and refData[index] < max(refData[index-1], refData[index+1]):\n",
    "            index = index + 1\n",
    "        index = index + 1\n",
    "        endIndex = index  \n",
    "        #print(endIndex)\n",
    "        if endIndex - startIndex > 20:\n",
    "            checkTime.append(endIndex - startIndex)\n",
    "\n",
    "        if endIndex < len(one) - 1 and endIndex - startIndex > 0.8*np.mean(checkTime) and endIndex - startIndex < 1.2*np.mean(checkTime):            \n",
    "            usedData = accNorm[startIndex: endIndex]\n",
    "            f = interp1d(range(len(usedData)), usedData, kind='cubic')\n",
    "            x = np.linspace(0, len(usedData)-1, num=NumOfSamples, endpoint=False)\n",
    "            processedData = f(x)\n",
    "            checkDataVar.append(np.var(processedData))\n",
    "            if(np.var(processedData) > 0.2 * np.mean(checkDataVar)):\n",
    "                oneTime.append(endIndex - startIndex)\n",
    "                checkData.append(processedData)\n",
    "                for i in range(6):\n",
    "                    usedData = one[startIndex: endIndex, i]\n",
    "                    f = interp1d(range(len(usedData)), usedData, kind='cubic')\n",
    "                    x = np.linspace(0, len(usedData)-1, num=NumOfSamples, endpoint=False)\n",
    "                    oneIMUdata[i].append(f(x))    \n",
    "                \n",
    "    IMUdata.append(oneIMUdata)\n",
    "    allTime.append(oneTime)\n",
    "    \n",
    "print(len(IMUdata), len(allTime))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.signal import find_peaks\n",
    "from scipy.signal import peak_widths\n",
    "\n",
    "\n",
    "def psdFeatures(one):    \n",
    "    re = [];\n",
    "    Fs = 100;\n",
    "    for i in range(3):\n",
    "        timeSeq = one[:, i] - np.mean(one[:, i])\n",
    "        N = len(timeSeq)\n",
    "        frqSeq = np.fft.fft(timeSeq)\n",
    "        frqSeqShift = np.fft.fftshift(frqSeq)\n",
    "        #print(type(frqSeq))\n",
    "        frqSeqHalf = frqSeq[:N//2+1]\n",
    "        psd = 1/(Fs*N) * abs(frqSeqHalf)**2\n",
    "        psd[2:] = psd[2:]*2\n",
    "        freq = np.linspace(0,Fs/2,N//2+1)\n",
    "        #plt.figure(i)\n",
    "        #plt.plot(freq, abs(psd), lw=2)\n",
    "        peaks, _ = find_peaks(psd)\n",
    "        peakWidth = peak_widths(psd, peaks, rel_height=0.5)\n",
    "        maxPeakIndex = np.argmax(psd[peaks])\n",
    "        re.append(psd[maxPeakIndex])\n",
    "        re.append(peakWidth[0][maxPeakIndex])\n",
    "        #print(peakWidth[0][maxPeakIndex]/np.amax(psd))\n",
    "        #print(peakWidth[0][maxPeakIndex]/(N//2+1)/psd[maxPeakIndex])\n",
    "        re.append(peakWidth[0][maxPeakIndex]/np.amax(psd))\n",
    "        re.append(freq[maxPeakIndex])\n",
    "    return re\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = label\n",
    "\n",
    "for i in range(len(IMUdata)):\n",
    "    oneX = []\n",
    "    #time related feature\n",
    "    oneX.append(len(allTime[i]))\n",
    "    oneX.append(np.mean(allTime[i]))\n",
    "    oneX.append(np.std(allTime[i]))\n",
    "    oneX.append(np.std(allTime[i])/(np.mean(allTime[i])))\n",
    "    \n",
    "    oneX = oneX + psdFeatures(rawData[i])\n",
    "    \n",
    "    #acc related feature\n",
    "    for j in range(6):\n",
    "        data = IMUdata[i][j]\n",
    "        oneX.append(np.mean(np.abs(np.mean(data, axis=0))))\n",
    "        oneX.append(np.std(np.abs(np.mean(data, axis=0))))\n",
    "        oneX.append(np.mean(np.std(data, axis=0)))\n",
    "        oneX.append(np.std(np.std(data, axis=0)))\n",
    "        oneX.append(np.mean(np.std(data, axis=0)/np.abs(np.mean(data, axis=0))))\n",
    "        oneX.append(np.std(np.std(data, axis=0)/np.abs(np.mean(data, axis=0))))\n",
    "\n",
    "    X.append(oneX)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "Xs = scaler.fit_transform(X)\n",
    "print(np.shape(np.array(X)))\n",
    "print(np.mean(X, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2).fit(Xs)\n",
    "pca_2d = pca.transform(Xs)\n",
    "\n",
    "\n",
    "import pylab as pl\n",
    "for i in range(0, pca_2d.shape[0]):\n",
    "    if y[i] == 0:\n",
    "        c1 = pl.scatter(pca_2d[i,0],pca_2d[i,1],c='r',    marker='+')\n",
    "    elif y[i] == 1:\n",
    "        c2 = pl.scatter(pca_2d[i,0],pca_2d[i,1],c='g',    marker='o')\n",
    "\n",
    "        \n",
    "pl.legend([c1, c2], ['Nonfaller', 'Faller'])\n",
    "pl.title('Training dataset with 2 classes')\n",
    "pl.show()\n",
    "\n",
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embedding(X, y, title=None):\n",
    "    x_min, x_max = np.min(X, 0), np.max(X, 0)\n",
    "    X = (X - x_min) / (x_max - x_min)     \n",
    "    plt.figure()\n",
    "    ax = plt.subplot(111)\n",
    "    for i in range(X.shape[0]):\n",
    "        plt.text(X[i, 0], X[i, 1], str(y[i]),\n",
    "                 color=plt.cm.Set1(y[i] / 10.),\n",
    "                 fontdict={'weight': 'bold', 'size': 9})\n",
    "    plt.xticks([]), plt.yticks([])\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "        \n",
    "from sklearn import manifold\n",
    "#降维\n",
    "tsne = manifold.TSNE(n_components=2, init='pca')\n",
    "\n",
    "X_tsne = tsne.fit_transform(X)\n",
    "#绘图\n",
    "plot_embedding(X_tsne,y*5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2).fit(X_train)\n",
    "\n",
    "#X_tf_pca = pca.transform(X_train)\n",
    "#X_tsf_pca = pca.transform(X_test)\n",
    "\n",
    "X_tf_pca = X_train\n",
    "X_tsf_pca = X_test\n",
    "\n",
    "\n",
    "#X_tf_pca = X_train\n",
    "#X_tsf_pca = X_test\n",
    "\n",
    "param_C = 1\n",
    "param_gamma = 0.00002\n",
    "\n",
    "classifier = svm.SVC(C=param_C,gamma=param_gamma,tol=0.0001)\n",
    "#classifier = svm.LinearSVC(C=param_C)\n",
    "\n",
    "#We learn the digits on train part\n",
    "start_time = dt.datetime.now()\n",
    "print('Start learning at {}'.format(str(start_time)))\n",
    "classifier.fit(X_tf_pca, y_train)\n",
    "#classifier.fit(X_pca, y)\n",
    "\n",
    "end_time = dt.datetime.now() \n",
    "print('Stop learning {}'.format(str(end_time)))\n",
    "elapsed_time= end_time - start_time\n",
    "print('Elapsed learning {}'.format(str(elapsed_time)))\n",
    "\n",
    "expected = y_train\n",
    "predicted = classifier.predict(X_tf_pca)\n",
    "print(expected)\n",
    "print(predicted)\n",
    "\n",
    "cm = metrics.confusion_matrix(expected, predicted)\n",
    "print(\"Confusion matrix:\\n%s\" % cm)\n",
    "print(\"Accuracy={}\".format(metrics.accuracy_score(expected, predicted)))\n",
    "\n",
    "\n",
    "expected = y_test\n",
    "predicted = classifier.predict(X_tsf_pca)\n",
    "print(expected)\n",
    "print(predicted)\n",
    "\n",
    "cm = metrics.confusion_matrix(expected, predicted)\n",
    "print(\"Confusion matrix:\\n%s\" % cm)\n",
    "print(\"Accuracy={}\".format(metrics.accuracy_score(expected, predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.1)\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "param_C = 1\n",
    "param_gamma = 0.00002\n",
    "\n",
    "classifier = svm.SVC(C=param_C,gamma=param_gamma,tol=0.001)\n",
    "scores = cross_val_score(classifier, X_train, y_train, cv=10)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "#We learn the digits on train part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "\n",
    "\n",
    "recall = []\n",
    "precision = []\n",
    "f1 = []\n",
    "\n",
    "\n",
    "\n",
    "pca = PCA(n_components=40).fit(X_train)\n",
    "#X_tf_pca = pca.transform(X_train)\n",
    "#X_tsf_pca = pca.transform(X_test)\n",
    "\n",
    "X_tf_pca = X_train\n",
    "X_tsf_pca = X_test\n",
    "\n",
    "for i in range(100):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.0001)\n",
    "    \n",
    "    \n",
    "    \n",
    "    pca = PCA(n_components=51).fit(X_train)\n",
    "    X_tf_pca = pca.transform(X_train)\n",
    "    X_tsf_pca = pca.transform(X_test)\n",
    "\n",
    "    #X_tf_pca = X_train\n",
    "    #X_tsf_pca = X_test\n",
    "    \n",
    "    scoring = ['precision', 'recall', 'f1']\n",
    "\n",
    "    param_C = 0.5\n",
    "    param_gamma = 0.00001\n",
    "\n",
    "    classifier = svm.SVC(C=param_C,gamma=param_gamma,tol=0.001)\n",
    "\n",
    "    scores = cross_validate(classifier, X_tf_pca, y_train, scoring=scoring,\n",
    "                             cv=5)\n",
    "\n",
    "    recall.append(np.mean(scores['test_recall']))\n",
    "    precision.append(np.mean(scores['test_precision']))    \n",
    "    f1.append(np.mean(scores['test_f1']))\n",
    "print(np.mean(recall))\n",
    "print(np.mean(precision))                      \n",
    "print(np.mean(f1))                      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2)\n",
    "\n",
    "pca = PCA(n_components=40).fit(X_train)\n",
    "X_tf_pca = pca.transform(X_train)\n",
    "X_tsf_pca = pca.transform(X_test)\n",
    "\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=2, max_leaf_nodes=10000, n_jobs=-1)\n",
    "rnd_clf.fit(X_tf_pca, y_train)\n",
    "\n",
    "#expected = y_test\n",
    "#predicted = rnd_clf.predict(X_tsf_pca)\n",
    "\n",
    "expected = y_train\n",
    "predicted = rnd_clf.predict(X_tf_pca)\n",
    "\n",
    "print(expected)\n",
    "print(predicted)\n",
    "\n",
    "cm = metrics.confusion_matrix(expected, predicted)\n",
    "print(\"Confusion matrix:\\n%s\" % cm)\n",
    "\n",
    "print(\"Accuracy={}\".format(metrics.accuracy_score(expected, predicted)))\n",
    "\n",
    "\n",
    "expected = y_test\n",
    "predicted = rnd_clf.predict(X_tsf_pca)\n",
    "\n",
    "print(expected)\n",
    "print(predicted)\n",
    "\n",
    "cm = metrics.confusion_matrix(expected, predicted)\n",
    "print(\"Confusion matrix:\\n%s\" % cm)\n",
    "\n",
    "print(\"Accuracy={}\".format(metrics.accuracy_score(expected, predicted)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
